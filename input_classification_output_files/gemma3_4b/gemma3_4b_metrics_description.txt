Metrics Description for results stored in 'predicted_types_prompts_gpu_cpu_ram.jsonl' (located in '/content/drive/MyDrive/llm_def/input_classification/gemma3_4b')
==============================================================================================

This file describes the metrics found within the 'metrics' dictionary for each entry
in the JSONL output file generated by the main classification script.

Time/Token Metrics (Reported by Ollama):
----------------------------------------
- prompt_tokens:            Number of tokens in the input prompt processed by the model. (unit: tokens)
- prompt_duration_sec:      Time spent processing the prompt tokens. (unit: seconds)
- prompt_tokens_per_sec:    Rate of prompt token processing. (unit: tokens/second)
- eval_tokens:              Number of tokens generated by the model in the response. (unit: tokens)
- eval_duration_sec:        Time spent generating the response tokens. (unit: seconds)
- eval_tokens_per_sec:      Rate of response token generation (inference speed). (unit: tokens/second)
- total_duration_sec:       Total time taken for the ollama.chat request (includes prompt eval, generation, and overhead). (unit: seconds)

GPU Metrics (Measured via pynvml, assuming GPU 0):
-------------------------------------------------
- gpu_utilization_percent:  GPU utilization percentage captured *after* the Ollama request completed. Aims to reflect usage *during* the request. (unit: %)
- gpu_memory_used_mib_before: GPU memory used *before* the Ollama request for this prompt. (unit: MiB - MebiBytes)
- gpu_memory_used_mib_after:  GPU memory used *after* the Ollama request completed. (unit: MiB - MebiBytes)
- gpu_memory_total_mib:     Total GPU memory available on the device. (unit: MiB - MebiBytes)

CPU/RAM Metrics (Measured via psutil):
--------------------------------------
- cpu_utilization_percent:  System-wide CPU utilization percentage measured *after* the Ollama request completed. Represents CPU usage *since the last measurement* before the request. (unit: %)
- ram_used_mib_before:      System-wide RAM used *before* the Ollama request for this prompt. (unit: MiB - MebiBytes)
- ram_used_mib_after:       System-wide RAM used *after* the Ollama request completed. (unit: MiB - MebiBytes)
- ram_total_mib:          Total physical RAM available on the system. (unit: MiB - MebiBytes)

Notes:
------
- 'MiB' (Mebibyte) = 1024 * 1024 bytes. 'GiB' (Gibibyte) = 1024 * 1024 * 1024 bytes.
- CPU and RAM metrics reflect system-wide usage, not just the Python script or Ollama process alone.
- GPU metrics typically refer to GPU index 0 unless modified in the main script.
- Utilization metrics captured 'after' the operation are snapshots and might not perfectly represent the peak usage during the entire operation.